{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "LETTERSTR = \"0123456789ABCDEFGHJKLMNPQRSTUVWXYZ\"\n",
    "batsize=10\n",
    "num_epochs=65\n",
    "def toonehot(text):\n",
    "    labellist = []\n",
    "    for letter in text:\n",
    "        onehot = [0 for _ in range(34)]\n",
    "        num = LETTERSTR.find(letter)\n",
    "        onehot[num] = 1\n",
    "        labellist.extend(onehot)  #every label size is 1*170, according with LETTERSTR\n",
    "#         labellist.append(onehot)\n",
    "    return labellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)]\n",
    "\n",
    "    for i in range(num_convs - 1):  # 定义后面的许多层\n",
    "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "    net.append(nn.BatchNorm2d(out_channels))\n",
    "    net.append(nn.MaxPool2d(2, 2))  # 定义池化层\n",
    "    net.append(nn.Dropout(0.3))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "\n",
    "# 将模型打印出来看一下结构\n",
    "block_demo = vgg_block(3, 3,256)\n",
    "# print(block_demo)\n",
    "\n",
    "# #首先定义输入为（1， 64， 300， 300）\n",
    "# input_demo = Variable(torch.zeros(1,128,7, 25))\n",
    "# print(input_demo.shape)\n",
    "# output_demo = block_demo(input_demo)\n",
    "# print(output_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面我们定义一个函数对这个 vgg block 进行堆叠\n",
    "def vgg_stack(num_convs, channels):\n",
    "    net = []\n",
    "#     print(zip(num_convs, channels))\n",
    "    for n, c in zip(num_convs, channels):\n",
    "        in_c = c[0]\n",
    "        out_c = c[1]\n",
    "        net.append(vgg_block(n, in_c, out_c))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "\n",
    "# 作为实例，我们定义一个稍微简单一点的 vgg 结构，其中有 8 个卷积层\n",
    "vgg_net = vgg_stack((2, 2, 2, 1), ((3, 32), (32, 64), (64, 128), (128, 256)))\n",
    "# print(vgg_net)\n",
    "\n",
    "#我们可以看到网络结构中有个 5 个 最大池化，说明图片的大小会减少 5 倍，我们可以验证一下，输入一张 256 x 256\n",
    "#的图片看看结果是什么\n",
    "test_x = Variable(torch.zeros(1, 3, 60, 200))\n",
    "test_y = vgg_net(test_x)\n",
    "# print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = vgg_net\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3*12*256, 2560),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2560, 170)# 34*5=170\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # 然后我们可以训练我们的模型看看在 cifar10 上的效果\n",
    "    def data_tf(x):\n",
    "        x = np.array(x, dtype='float32') / 255\n",
    "        x = (x - 0.5) / 0.5\n",
    "        x = x.transpose((2, 0, 1))  ## 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
    "        x = torch.from_numpy(x)\n",
    "        return x\n",
    "    \n",
    "net = vgg()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Reading training data...\nShape of train data: (9999, 3, 60, 200)\nReading validation data...\nShape of validation data: (200, 3, 60, 200)\n"
    }
   ],
   "source": [
    "print(\"Reading training data...\")\n",
    "traincsv = open('data/5_imitate_train_set/captcha_train.csv', 'r', encoding = 'utf8')\n",
    "train_data = np.stack([np.array(cv2.imread(\"data/5_imitate_train_set/\" + row[0] + \".jpg\"))/255.0 for row in csv.reader(traincsv)])\n",
    "train_data = train_data.transpose(0,3,1,2)\n",
    "traincsv = open('data/5_imitate_train_set/captcha_train.csv', 'r', encoding = 'utf8')    \n",
    "train_label = [toonehot(row[1]) for row in csv.reader(traincsv)]\n",
    "# print(train_label[0:2])\n",
    "print(\"Shape of train data:\", train_data.shape)\n",
    "# print(\"Shape of train label:\", len(train_label))\n",
    "\n",
    "print(\"Reading validation data...\")\n",
    "valicsv = open('data/5_imitate_vali_set/captcha_vali.csv', 'r', encoding = 'utf8')\n",
    "vali_data = np.stack([np.array(Image.open(\"./data/5_imitate_vali_set/\" + row[0] + \".jpg\"))/255.0 for row in csv.reader(valicsv)])\n",
    "vali_data = vali_data.transpose(0,3,1,2)\n",
    "valicsv = open('data/5_imitate_vali_set/captcha_vali.csv', 'r', encoding = 'utf8')\n",
    "vali_label = [toonehot(row[1]) for row in csv.reader(valicsv)]\n",
    "# vali_label = [[] for _ in range(5)]\n",
    "# for arr in read_label:\n",
    "#     for index in range(5):\n",
    "#         vali_label[index].append(arr[index])\n",
    "# vali_label = [arr for arr in np.asarray(vali_label)]\n",
    "print(\"Shape of validation data:\", vali_data.shape)\n",
    "# print(\"Shape of validation label:\", vali_label[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(output, label):\n",
    "    correct_num =0\n",
    "    for i in range(output.size()[0]):\n",
    "        c0 = np.argmax(output[i, 0:34].data.cpu().numpy())\n",
    "        c1 = np.argmax(output[i, 34:68].data.cpu().numpy())\n",
    "        c2 = np.argmax(output[i, 68:102].data.cpu().numpy())\n",
    "        c3 = np.argmax(output[i, 102:136].data.cpu().numpy())\n",
    "        c4 = np.argmax(output[i, 136:170].data.cpu().numpy())\n",
    "        c = '%s/%s/%s/%s/%s' % (c0, c1, c2, c3,c4)\n",
    "        l0 = np.argmax(label[i, 0:34].data.cpu().numpy())\n",
    "        l1 = np.argmax(label[i, 34:68].data.cpu().numpy())\n",
    "        l2 = np.argmax(label[i, 68:102].data.cpu().numpy())\n",
    "        l3 = np.argmax(label[i, 102:136].data.cpu().numpy())\n",
    "        l4 = np.argmax(label[i, 136:170].data.cpu().numpy())\n",
    "        l = '%s/%s/%s/%s/%s' % (l0, l1, l2, l3,l4)\n",
    "        if l==c:\n",
    "            correct_num += 1        \n",
    "    return float(correct_num)/ len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch    0. Train Loss: 0.015149, Train Acc: 0.000000, Time 00:01:44\nEpoch    1. Train Loss: 0.011305, Train Acc: 0.000200, Time 00:01:43\nEpoch    2. Train Loss: 0.008697, Train Acc: 0.014511, Time 00:01:43\nEpoch    3. Train Loss: 0.005528, Train Acc: 0.180211, Time 00:01:43\nEpoch    4. Train Loss: 0.003443, Train Acc: 0.468578, Time 00:01:43\nEpoch    5. Train Loss: 0.002295, Train Acc: 0.659078, Time 00:01:43\nEpoch    6. Train Loss: 0.001760, Train Acc: 0.741778, Time 00:01:43\nEpoch    7. Train Loss: 0.001321, Train Acc: 0.822800, Time 00:01:43\nEpoch    8. Train Loss: 0.001129, Train Acc: 0.864900, Time 00:01:43\nEpoch    9. Train Loss: 0.001151, Train Acc: 0.849700, Time 00:01:43\nEpoch   10. Train Loss: 0.000889, Train Acc: 0.895100, Time 00:01:43\nEpoch   11. Train Loss: 0.000843, Train Acc: 0.899800, Time 00:01:43\nEpoch   12. Train Loss: 0.000868, Train Acc: 0.906300, Time 00:01:43\nEpoch   13. Train Loss: 0.000712, Train Acc: 0.926189, Time 00:01:42\nEpoch   14. Train Loss: 0.000694, Train Acc: 0.930400, Time 00:01:42\nEpoch   15. Train Loss: 0.000684, Train Acc: 0.936289, Time 00:01:42\nEpoch   16. Train Loss: 0.000956, Train Acc: 0.900689, Time 00:01:43\nEpoch   17. Train Loss: 0.000551, Train Acc: 0.949600, Time 00:01:43\nEpoch   18. Train Loss: 0.000659, Train Acc: 0.939978, Time 00:01:44\nEpoch   19. Train Loss: 0.000581, Train Acc: 0.951800, Time 00:01:42\nEpoch   20. Train Loss: 0.000544, Train Acc: 0.959589, Time 00:01:43\nEpoch   21. Train Loss: 0.000562, Train Acc: 0.956889, Time 00:01:43\nEpoch   22. Train Loss: 0.000764, Train Acc: 0.931089, Time 00:01:42\nEpoch   23. Train Loss: 0.000546, Train Acc: 0.956000, Time 00:01:43\nEpoch   24. Train Loss: 0.000475, Train Acc: 0.967200, Time 00:01:42\nEpoch   25. Train Loss: 0.000536, Train Acc: 0.964600, Time 00:01:45\nEpoch   26. Train Loss: 0.000522, Train Acc: 0.966100, Time 00:01:43\nEpoch   27. Train Loss: 0.000577, Train Acc: 0.964400, Time 00:01:43\nEpoch   28. Train Loss: 0.000513, Train Acc: 0.969000, Time 00:01:43\nEpoch   29. Train Loss: 0.000503, Train Acc: 0.970389, Time 00:01:42\nEpoch   30. Train Loss: 0.000504, Train Acc: 0.968800, Time 00:01:42\nEpoch   31. Train Loss: 0.000564, Train Acc: 0.967500, Time 00:01:42\nEpoch   32. Train Loss: 0.000501, Train Acc: 0.973100, Time 00:01:44\nEpoch   33. Train Loss: 0.000571, Train Acc: 0.970800, Time 00:01:45\nEpoch   34. Train Loss: 0.000461, Train Acc: 0.974700, Time 00:01:43\nEpoch   35. Train Loss: 0.000584, Train Acc: 0.969889, Time 00:01:43\nEpoch   36. Train Loss: 0.000564, Train Acc: 0.969200, Time 00:01:43\nEpoch   37. Train Loss: 0.000463, Train Acc: 0.976800, Time 00:01:44\nEpoch   38. Train Loss: 0.000852, Train Acc: 0.967789, Time 00:01:46\nEpoch   39. Train Loss: 0.000461, Train Acc: 0.979900, Time 00:01:45\nEpoch   40. Train Loss: 0.000405, Train Acc: 0.983189, Time 00:01:45\nEpoch   41. Train Loss: 0.000446, Train Acc: 0.983800, Time 00:01:44\nEpoch   42. Train Loss: 0.000572, Train Acc: 0.975000, Time 00:01:44\nEpoch   43. Train Loss: 0.000656, Train Acc: 0.970900, Time 00:01:46\nEpoch   44. Train Loss: 0.000442, Train Acc: 0.981200, Time 00:01:46\nEpoch   45. Train Loss: 0.000473, Train Acc: 0.981800, Time 00:01:44\nEpoch   46. Train Loss: 0.000464, Train Acc: 0.985300, Time 00:01:44\nEpoch   47. Train Loss: 0.000531, Train Acc: 0.981100, Time 00:01:49\nEpoch   48. Train Loss: 0.000534, Train Acc: 0.979900, Time 00:01:47\nEpoch   49. Train Loss: 0.000507, Train Acc: 0.981300, Time 00:01:48\nEpoch   50. Train Loss: 0.000538, Train Acc: 0.982200, Time 00:01:45\nEpoch   51. Train Loss: 0.000511, Train Acc: 0.983800, Time 00:01:44\nEpoch   52. Train Loss: 0.000519, Train Acc: 0.983000, Time 00:01:44\nEpoch   53. Train Loss: 0.000576, Train Acc: 0.982100, Time 00:01:44\nEpoch   54. Train Loss: 0.000478, Train Acc: 0.984800, Time 00:01:45\nEpoch   55. Train Loss: 0.000540, Train Acc: 0.982400, Time 00:01:45\nEpoch   56. Train Loss: 0.000486, Train Acc: 0.987200, Time 00:01:44\nEpoch   57. Train Loss: 0.000710, Train Acc: 0.981100, Time 00:01:44\nEpoch   58. Train Loss: 0.001035, Train Acc: 0.978500, Time 00:01:44\nEpoch   59. Train Loss: 0.000567, Train Acc: 0.985700, Time 00:01:44\nEpoch   60. Train Loss: 0.000949, Train Acc: 0.969700, Time 00:01:44\nEpoch   61. Train Loss: 0.000437, Train Acc: 0.987400, Time 00:01:44\nEpoch   62. Train Loss: 0.000476, Train Acc: 0.988100, Time 00:01:44\nEpoch   63. Train Loss: 0.000493, Train Acc: 0.986889, Time 00:01:44\nEpoch   64. Train Loss: 0.000551, Train Acc: 0.985400, Time 00:01:44\n"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "prev_time = datetime.datetime.now()\n",
    "iters = int(math.ceil(train_data.shape[0]/batsize)) \n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net = net.train()              \n",
    "    for i in range(iters):\n",
    "        temp_acc = 0\n",
    "        im = train_data[i*batsize: (i+1)*batsize]\n",
    "        im = torch.tensor(im).float()\n",
    "        label = train_label[i*batsize: (i+1)*batsize]\n",
    "        label = torch.tensor(label).float()\n",
    "        if torch.cuda.is_available():                    \n",
    "            im = Variable(im.cuda())\n",
    "            label = Variable(label.cuda())\n",
    "        else:\n",
    "            im = Variable(im)\n",
    "            label = Variable(label)\n",
    "        # forward\n",
    "        output = net(im)\n",
    "#         print(output.shape)\n",
    "#         print(label.shape)\n",
    "        loss = criterion(output, label)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "#         print(output.shape)\n",
    "#         print(label.shape)\n",
    "        train_acc += get_acc(output, label)\n",
    "        \n",
    "    cur_time = datetime.datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "    prev_time = cur_time\n",
    "    epoch_str = (\"Epoch %4d. Train Loss: %f, Train Acc: %f, \" %\n",
    "                     (epoch, train_loss / len(train_data),\n",
    "                      train_acc / iters))\n",
    "    print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type vgg. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n"
    }
   ],
   "source": [
    "torch.save(net,'./models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vali_acc(output, label):\n",
    "    correct_num =0\n",
    "    for i in range(output.size()[0]):\n",
    "        c0 = np.argmax(output[i, 0:34].data.cpu().numpy())\n",
    "        c1 = np.argmax(output[i, 34:68].data.cpu().numpy())\n",
    "        c2 = np.argmax(output[i, 68:102].data.cpu().numpy())\n",
    "        c3 = np.argmax(output[i, 102:136].data.cpu().numpy())\n",
    "        c4 = np.argmax(output[i, 136:170].data.cpu().numpy())\n",
    "        c = '%s/%s/%s/%s/%s' % (c0, c1, c2, c3,c4)\n",
    "        l0 = np.argmax(label[i, 0:34].data.cpu().numpy())\n",
    "        l1 = np.argmax(label[i, 34:68].data.cpu().numpy())\n",
    "        l2 = np.argmax(label[i, 68:102].data.cpu().numpy())\n",
    "        l3 = np.argmax(label[i, 102:136].data.cpu().numpy())\n",
    "        l4 = np.argmax(label[i, 136:170].data.cpu().numpy())\n",
    "        l = '%s/%s/%s/%s/%s' % (l0, l1, l2, l3,l4)\n",
    "        print(c,l)\n",
    "        if l==c:\n",
    "            correct_num += 1        \n",
    "    return float(correct_num)/ len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6/29/1/28/6 6/29/1/28/6\n6/20/28/31/4 6/20/28/31/4\n27/31/2/11/7 27/31/2/11/7\n20/7/26/14/18 20/7/26/14/18\n17/11/2/11/8 17/11/2/11/8\n3/27/32/6/0 3/27/29/6/0\n24/21/21/19/11 24/21/21/19/11\n33/19/19/10/21 33/19/19/10/21\n20/30/16/7/10 20/30/16/7/10\n14/3/0/12/25 14/3/0/12/25\n22/2/1/2/27 22/2/1/2/27\n7/6/32/3/20 7/6/32/3/20\n0/23/22/6/13 0/23/22/6/13\n23/10/16/30/17 23/10/16/30/17\n7/20/15/6/14 7/20/15/6/14\n5/25/17/18/24 5/25/17/18/24\n29/15/8/20/20 29/15/8/20/20\n31/21/15/10/17 31/21/15/10/17\n21/16/24/27/5 21/16/24/27/5\n16/1/11/11/27 16/1/11/11/27\n28/21/9/17/31 28/21/9/17/31\n18/27/10/14/28 18/27/10/14/28\n10/16/7/30/8 10/16/7/30/8\n30/13/4/31/10 30/13/4/31/10\n3/8/21/29/22 3/8/21/29/22\n8/22/29/11/22 8/22/29/11/22\n32/5/12/13/23 32/5/12/13/23\n4/10/11/9/23 4/10/11/9/23\n31/33/23/0/25 31/33/23/0/25\n19/4/7/1/31 19/4/7/1/31\n19/12/10/31/15 19/12/10/31/15\n9/6/24/5/29 9/6/24/5/29\n17/26/30/23/28 17/26/30/23/28\n33/24/16/23/9 33/24/16/23/9\n25/32/21/2/22 25/32/21/2/22\n14/3/12/7/7 14/3/12/7/29\n19/7/29/12/6 19/7/29/12/6\n13/5/26/22/33 13/5/26/22/33\n19/14/3/7/20 19/14/3/7/20\n23/16/11/6/31 23/16/11/6/31\n11/0/12/32/10 11/0/12/32/10\n22/28/33/5/21 22/28/33/5/21\n2/6/16/23/26 2/6/16/23/26\n5/26/24/6/28 5/26/24/6/28\n14/27/14/26/28 14/27/14/26/28\n6/0/3/6/2 6/0/3/6/2\n24/2/14/27/11 24/2/14/27/11\n8/21/23/0/6 8/21/23/0/6\n23/10/22/14/3 23/10/22/14/3\n30/6/29/30/33 30/6/29/30/33\n27/15/13/10/15 27/15/13/10/15\n2/13/1/31/12 2/13/1/31/12\n11/13/30/12/10 11/13/30/12/10\n28/0/27/27/26 28/0/27/27/26\n18/24/28/29/19 18/24/28/29/19\n2/5/27/8/19 2/5/27/8/24\n24/2/26/14/24 24/2/26/14/24\n21/3/10/24/9 21/3/10/24/9\n14/17/11/32/32 14/17/11/32/32\n25/14/29/9/16 25/14/29/9/16\n15/3/14/22/2 15/3/14/22/2\n18/1/7/18/27 18/1/7/18/27\n19/20/32/2/17 19/20/32/2/17\n29/33/12/11/30 29/33/12/11/30\n17/16/15/29/26 17/16/15/29/26\n19/17/17/12/6 19/17/17/12/6\n32/18/4/3/0 32/18/4/3/0\n1/32/16/29/16 1/32/16/29/16\n24/12/14/28/30 24/12/14/28/30\n8/15/15/9/2 8/15/15/9/2\n21/29/13/11/13 21/29/13/11/13\n25/22/20/11/20 25/22/20/11/20\n0/8/9/15/3 0/8/9/15/3\n28/16/9/0/18 28/16/9/0/18\n30/33/14/12/29 30/33/14/12/29\n22/24/2/18/25 22/24/2/18/25\n23/5/23/14/6 23/5/23/14/6\n1/2/8/8/5 1/2/8/8/5\n5/1/1/17/10 5/1/1/17/19\n3/13/13/2/8 3/13/13/2/8\n9/22/2/28/12 9/22/2/28/12\n26/33/11/7/30 26/33/11/7/30\n21/30/23/25/30 21/30/23/25/30\n12/3/20/19/33 12/3/20/19/33\n10/16/14/20/7 10/16/14/20/7\n31/10/29/21/0 31/10/29/21/0\n22/31/12/18/25 22/31/12/18/25\n15/25/28/25/12 15/25/28/25/12\n28/21/11/13/7 28/21/11/13/7\n30/4/10/26/2 30/4/10/26/2\n18/3/7/12/10 18/3/7/12/10\n22/26/3/13/0 22/26/3/13/0\n10/31/26/16/9 10/31/26/16/9\n28/27/20/20/15 28/27/20/20/15\n11/2/23/9/20 11/2/23/9/20\n4/32/21/32/19 4/32/21/32/19\n7/11/10/12/12 7/11/10/12/12\n26/17/29/21/24 26/17/29/21/24\n0/3/29/11/20 0/3/29/24/20\n21/25/16/9/7 21/25/16/9/7\n23/4/9/4/13 23/4/9/4/13\n25/3/12/9/21 25/3/12/9/21\n12/29/32/2/23 12/29/32/2/23\n0/30/6/4/1 0/30/6/4/1\n11/16/20/13/10 11/16/20/13/10\n22/22/3/8/32 22/22/3/8/32\n0/7/17/13/16 0/7/17/13/16\n26/6/32/31/32 26/6/32/31/32\n5/3/4/23/5 5/3/4/23/5\n14/11/1/24/4 14/11/1/24/4\n26/23/28/12/0 26/23/28/12/0\n16/22/6/29/33 16/22/6/29/33\n5/31/22/24/23 3/31/22/24/23\n1/29/22/10/33 1/29/22/10/33\n13/5/32/25/7 13/5/32/25/7\n9/22/17/12/0 9/22/17/12/0\n17/15/4/30/25 17/15/4/30/25\n1/1/22/11/27 1/1/22/11/27\n19/11/7/31/10 19/11/7/31/10\n10/20/7/0/21 10/20/7/0/21\n17/10/26/15/8 17/10/26/15/8\n8/16/15/12/14 8/16/15/12/14\n32/26/0/13/26 32/26/0/13/26\n6/23/17/25/32 6/23/17/25/32\n26/20/23/16/7 26/20/23/16/7\n25/9/6/14/2 25/9/6/14/2\n13/19/27/15/14 13/19/27/15/14\n12/19/13/5/22 12/19/13/5/22\n11/12/19/32/24 11/12/19/32/24\n8/6/25/21/20 8/6/25/21/20\n11/14/1/14/15 11/14/1/14/15\n32/17/11/17/27 32/17/11/17/27\n17/21/19/18/11 17/21/19/18/11\n23/33/18/12/32 23/33/18/12/32\n30/1/18/2/33 30/1/18/2/33\n28/24/4/7/7 28/24/4/7/7\n5/21/5/12/26 5/21/5/12/26\n2/20/11/10/1 2/20/11/10/1\n33/17/19/3/3 33/17/19/3/28\n30/14/4/31/10 30/14/4/31/10\n26/1/26/23/32 26/1/26/23/32\n28/31/29/19/10 28/31/29/19/10\n13/12/14/0/29 13/12/14/0/29\n26/7/25/12/1 26/7/27/25/1\n20/2/1/2/9 20/2/1/2/9\n14/30/6/2/30 14/30/6/2/30\n24/12/29/23/8 24/12/29/23/8\n3/8/19/24/20 3/8/19/24/20\n25/20/14/14/31 25/20/14/14/31\n33/29/28/8/33 33/29/28/8/33\n2/1/25/11/16 2/1/25/11/16\n16/32/27/28/10 16/32/27/28/10\n25/18/12/12/12 25/18/12/12/12\n11/6/0/4/32 11/6/0/4/32\n8/18/5/12/18 8/18/5/12/18\n27/23/33/27/31 27/23/33/27/31\n11/6/24/27/18 11/6/24/27/18\n30/13/2/26/4 30/13/26/26/4\n11/33/11/25/9 11/33/11/25/9\n26/9/2/12/23 26/9/2/12/23\n16/27/1/28/25 16/27/1/28/25\n29/13/14/21/8 29/13/14/21/8\n14/22/10/29/14 14/22/10/29/14\n31/26/28/23/24 31/26/28/23/24\n26/20/27/27/8 26/20/27/27/8\n17/0/31/3/28 17/0/31/3/28\n21/10/17/22/27 21/10/17/22/27\n16/8/33/32/14 16/8/33/32/14\n29/13/26/2/17 29/13/26/2/17\n17/6/2/6/17 17/6/2/6/17\n10/6/31/2/9 10/6/31/2/9\n11/11/29/25/33 11/11/29/25/33\n14/9/1/23/27 14/9/1/23/27\n26/26/20/9/27 26/26/20/9/27\n2/25/2/32/31 2/25/2/32/31\n14/8/8/23/31 14/8/8/23/31\n29/31/6/6/31 29/31/6/6/31\n3/12/26/12/2 3/12/26/12/2\n6/33/14/15/21 6/33/14/15/21\n32/21/1/30/20 32/21/1/30/20\n2/33/21/1/14 2/33/21/1/14\n22/8/6/16/10 10/8/6/16/10\n7/1/15/16/21 7/1/15/16/21\n24/8/29/8/8 24/8/29/8/8\n12/19/3/10/6 12/19/3/10/12\n25/25/13/1/9 25/25/13/1/9\n12/1/14/12/12 12/32/14/12/12\n1/25/5/27/15 1/25/5/27/15\n18/7/30/21/25 18/7/30/21/25\n0/22/2/23/4 0/22/4/23/4\n9/22/7/17/11 9/22/7/17/11\n14/16/13/9/6 14/16/13/9/6\n9/0/16/12/23 9/0/16/12/23\n20/10/8/10/6 20/10/8/10/6\n19/22/10/11/11 19/22/10/21/11\n15/8/6/10/25 15/8/6/10/25\n20/22/3/13/27 20/22/3/13/27\n31/2/21/4/23 31/2/21/4/23\n20/19/29/22/30 20/19/29/22/30\n25/0/0/28/3 25/0/0/28/3\n"
    }
   ],
   "source": [
    "net.eval()\n",
    "vali_acc = 0\n",
    "for i in range(int(math.ceil(len(vali_data)/batsize))):\n",
    "    im = vali_data[i*batsize: (i+1)*batsize]\n",
    "    im = torch.tensor(im).float()\n",
    "    label = vali_label[i*batsize: (i+1)*batsize]\n",
    "    label = torch.tensor(label).float()\n",
    "    if torch.cuda.is_available():                    \n",
    "        im = Variable(im.cuda())\n",
    "        label = Variable(label.cuda())\n",
    "    else:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "    output = net(im)\n",
    "    vali_acc += get_vali_acc(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Vali Acc: 0.930000\n"
    }
   ],
   "source": [
    "vali_acc = vali_acc/int(math.ceil(len(vali_data)/batsize))\n",
    "epoch_str = (\"Vali Acc: %f\"%vali_acc)\n",
    "print(epoch_str)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}